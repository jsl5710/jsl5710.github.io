---
title: AI Robustness & Adversarial Safety
summary: Investigating how dialect diversity, authorship obfuscation, and expert-level text editing expose critical vulnerabilities in content detection systems.
tags:
  - Adversarial ML
  - AI Robustness
  - Content Moderation
  - Dialect Robustness
date: '2026-02-01T00:00:00Z'

external_link: ''

image:
  caption: ''
  focal_point: Smart
---

AI systems deployed in the real world must withstand adversarial manipulation and perform reliably across the full spectrum of human language variation. This project investigates how dialect diversity, authorship obfuscation, and expert-level text editing expose critical vulnerabilities in content detection systems. From stress-testing harmful content classifiers across 50 English dialects to evaluating robustness against sophisticated evasion techniques, this work reveals that the Digital Language Divide is not only a gap in language coverage but also a security vulnerability—one that adversaries can exploit when AI systems are brittle to linguistic variation.

**Related Publications:**

- **DIA-HARM** (2026) — Harmful content detection robustness across 50 dialects
- **Authorship Obfuscation in Multilingual MGT Detection** (2024, EMNLP)
- **BEEMO** (2025, NAACL) — Expert-edited machine-generated outputs benchmark
